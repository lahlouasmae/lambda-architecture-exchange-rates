from pyspark.sql import SparkSession
import sys
import os

# =========================================================
# 1. CrÃ©ation de la Spark Session
# =========================================================
spark = SparkSession.builder \
    .appName("Verify Batch Layer Outputs") \
    .getOrCreate()

spark.sparkContext.setLogLevel("ERROR")

# Chemin de base alignÃ© avec le batch layer
base_path = "hdfs://hadoop-namenode:8020/user/spark/exchange_rates"

print("\n" + "=" * 70)
print("ðŸ” VÃ‰RIFICATION DU BATCH LAYER â€“ RÃ‰SULTATS DE SORTIE")
print("=" * 70)


# =========================================================
# 2. Fonction gÃ©nÃ©rique de vÃ©rification
# =========================================================
def verify_dataset(relative_path, format_name, logical_name):
    """
    VÃ©rifie l'existence, le volume et la structure d'un dataset Batch.
    Retourne (success: bool, count: int, df)
    """
    full_path = os.path.join(base_path, relative_path)

    try:
        print("\n" + "-" * 70)
        print(f"ðŸ“¦ Dataset : {logical_name}")
        print(f"ðŸ“ Format  : {format_name.upper()}")
        print("-" * 70)

        df = spark.read.format(format_name).load(full_path)
        count = df.count()

        if count == 0:
            print("âš  Dataset prÃ©sent mais vide")
            return True, 0, df

        print("âœ… Dataset disponible et lisible")
        print(f"âœ” Nombre total dâ€™enregistrements : {count}")
        print(f"âœ” Nombre de colonnes             : {len(df.columns)}")

        print("\nðŸ“ Structure des donnÃ©es :")
        df.printSchema()

        # AperÃ§u limitÃ© pour rapport
        print("\nðŸ“ AperÃ§u (5 lignes) :")
        df.show(5, truncate=False)

        return True, count, df

    except Exception as e:
        print("âŒ Dataset indisponible ou illisible")
        print(f"âš  DÃ©tail technique : {str(e).splitlines()[0]}")
        return False, 0, None


# =========================================================
# 3. VÃ©rification des sorties du Batch Layer
# =========================================================

# Master Data (AVRO)
master_ok, master_count, df_master = verify_dataset(
    relative_path="master",
    format_name="avro",
    logical_name="Master Data (donnÃ©es brutes normalisÃ©es)"
)

# Batch View (PARQUET)
batch_ok, batch_count, df_batch = verify_dataset(
    relative_path="daily_avg",
    format_name="parquet",
    logical_name="Batch View (agrÃ©gation journaliÃ¨re)"
)


# =========================================================
# 4. VÃ©rification logique et rÃ©sumÃ©
# =========================================================
print("\n" + "=" * 70)
print("ðŸ“‹ RÃ‰SUMÃ‰ DU BATCH LAYER")
print("=" * 70)

print(f"âœ” Master Data (AVRO) : {'OK' if master_ok else 'NON DISPONIBLE'} ({master_count} enregistrements)")
print(f"âœ” Batch View (PARQUET) : {'OK' if batch_ok else 'NON DISPONIBLE'} ({batch_count} enregistrements)")

# Comparaison des colonnes si les deux existent
if master_ok and batch_ok and master_count > 0 and batch_count > 0:
    print("\nðŸ”„ COHÃ‰RENCE ENTRE MASTER DATA ET BATCH VIEW")
    master_cols = set(df_master.columns)
    batch_cols = set(df_batch.columns)
    common_cols = master_cols.intersection(batch_cols)
    print(f"âœ” Colonnes communes             : {sorted(common_cols)}")
    print(f"âœ” Colonnes Master uniquement    : {sorted(master_cols - batch_cols)}")
    print(f"âœ” Colonnes Batch View uniquement: {sorted(batch_cols - master_cols)}")

    # Exemple de requÃªtes
    print("\nðŸ“Š Exemple : taux moyens par devise (Batch View)")
    df_batch.orderBy("rate_date", "target_currency").show(5, truncate=False)

    print("\nðŸ“Š Exemple : nombre de taux par devise (Master Data)")
    df_master.groupBy("target_currency").count().show(5, truncate=False)

    print("\nâœ” CohÃ©rence globale  : VALIDÃ‰E")
else:
    print("\nâš  CohÃ©rence globale  : Ã€ VÃ‰RIFIER")

print("=" * 70)
print("\nâœ… VÃ©rification du Batch Layer terminÃ©e !")
spark.stop()
sys.exit(0)